{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Example:\n",
    "\n",
    "This example reproduces the sentiment analysis with the fast text classifier described in [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759). In the papaer, the sentences are represent as bag of words (BoW) and train a linear classifier. Recently, a few text classification datasets, including \n",
    "    - AG_NEWS,\n",
    "    - SogouNews, \n",
    "    - DBpedia, \n",
    "    - YelpReviewPolarity,\n",
    "    - YelpReviewFull, \n",
    "    - YahooAnswers, \n",
    "    - AmazonReviewPolarity,\n",
    "    - AmazonReviewFull\n",
    "are added to PyTorch/torchtext and can be loaded with a single command. This example shows the applciation of TextClassificationDataset and reproduce the results of the paper.\n",
    "\n",
    "## Load data with ngrams\n",
    "A bag of ngrams features is used in the paper to capture some partial information about the local word order. In practice, bi-gram or tri-gram are applied to provide more benefits as word groups than only one word. An example:\n",
    "\n",
    "    \"load data with ngrams\"\n",
    "    Bi-grams results: \"load data\", \"data with\", \"with ngrams\"\n",
    "    Tri-grams results: \"load data with\", \"data with ngrams\"\n",
    "\n",
    "TextClassificationDataset supports the ngrams method. By setting ngrams to 2, the example text in the dataset will be a list of single words plus bi-grams string.\n",
    "\n",
    "Data iterators are loaded via the iters() function in the instance with a batch size of 128 and computation device. At the same time, the word strings are numericalized (i.e. converted from a list of tokens to a list of indexs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "NGRAMS = 2\n",
    "txt_cls = torchtext.datasets.AG_NEWS(ngrams=NGRAMS)\n",
    "BATCH_SIZE = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_iterator, test_iterator, valid_iterator = txt_cls.iters(batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "The model is composed of the embedding layer and the linear layer. Between the two layers, we apply a 2D average pooling function over the input signal (see [avg_pool2d](https://pytorch.org/docs/stable/nn.html?highlight=avg_pool2d#torch.nn.AvgPool2d)). Then, we use the log-softmax function to compute the probability distribution over the classes.\n",
    "<img src=\"./pictures/text_sentiment_ngrams_model.png\" width=\"600\" height=\"360\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text) # Sent_Len * Batch_Size * Embed_Dim\n",
    "        embedded = embedded.transpose(0, 1) # Batch_Size * Sent_Len * Embed_Dim\n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) # Batch_Size * Embed_Dim\n",
    "        out = self.fc(pooled) # Batch_Size * Num_Class\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate an instance\n",
    "\n",
    "The AG_NEWS dataset has four labels and therefore the number of classes is four.\n",
    "\n",
    "    1 : World\n",
    "    2 : Sports\n",
    "    3 : Business\n",
    "    4 : Sci/Tec\n",
    "\n",
    "The vocab size is equal to the length of vocab (including single word and ngrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(txt_cls.fields['text'].vocab)\n",
    "EMBED_DIM = 128\n",
    "NUN_CLASS = 4\n",
    "PAD_IDX = txt_cls.fields['text'].vocab.stoi[txt_cls.fields['text'].pad_token]\n",
    "UNK_IDX = txt_cls.fields['text'].vocab.stoi[txt_cls.fields['text'].unk_token]\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS, PAD_IDX).to(device)\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBED_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBED_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Add optimizer and loss function\n",
    "\n",
    "[Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) algorithm is used to optimize the model. \n",
    "[NLLLoss](https://pytorch.org/docs/stable/nn.html?highlight=nll_loss#torch.nn.NLLLoss) function calculates the negative log likelihood loss, which is used to train a classification problem with C classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_func = F.nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "## Define a function to train the model and evaluate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_evaluate_func(model, iterator, loss_func, status='train', optimizer=None):\n",
    "    \n",
    "    _loss = 0\n",
    "    _acc = 0\n",
    "    \n",
    "    if status == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for batch in iterator:\n",
    "        if status == 'train':\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        output = model(batch.text)\n",
    "        target = batch.label.long()\n",
    "        \n",
    "        loss = loss_func(output, target)\n",
    "                \n",
    "        if status == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        _loss += loss.item()\n",
    "            \n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        acc = pred.eq(target.view_as(pred)).sum().item() / len(target)\n",
    "        _acc += acc \n",
    "    return _loss / len(iterator), _acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "N_EPOCHS = 6\n",
    "min_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_evaluate_func(model, train_iterator, loss_func, status='train', optimizer=optimizer)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = train_evaluate_func(model, valid_iterator, loss_func, status='valid')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    _mins = int((end_time - start_time) / 60)\n",
    "    _secs = int(end_time - start_time - _mins * 60)\n",
    "\n",
    "    if valid_loss < min_valid_loss:\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(model, 'text_classification.pt')\n",
    "    \n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(_mins, _secs))\n",
    "    print(f'\\tLoss: {train_loss:.3f}(train)\\t|\\t{valid_loss:.3f}(valid)')\n",
    "    print(f'\\tAcc: {train_acc * 100:.1f}%(train)\\t|\\t{valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model on GPU with the following information:\n",
    "\n",
    "Epoch: 1  | time in 0 minutes, 16 seconds\n",
    "\n",
    "        Loss: 0.963(train)      |       0.362(valid)\n",
    "        Acc: 80.5%(train)       |       88.5%(valid)\n",
    "        \n",
    "Epoch: 2  | time in 0 minutes, 16 seconds\n",
    "\n",
    "        Loss: 0.375(train)      |       0.349(valid)\n",
    "        Acc: 91.7%(train)       |       90.6%(valid)\n",
    "        \n",
    "Epoch: 3  | time in 0 minutes, 16 seconds\n",
    "\n",
    "        Loss: 0.228(train)      |       0.353(valid)\n",
    "        Acc: 94.5%(train)       |       91.4%(valid)\n",
    "        \n",
    "Epoch: 4  | time in 0 minutes, 16 seconds\n",
    "\n",
    "        Loss: 0.151(train)      |       0.359(valid)\n",
    "        Acc: 96.4%(train)       |       91.8%(valid)\n",
    "        \n",
    "Epoch: 5  | time in 0 minutes, 16 seconds\n",
    "\n",
    "        Loss: 0.102(train)      |       0.373(valid)\n",
    "        Acc: 97.6%(train)       |       91.9%(valid)\n",
    "        \n",
    "Epoch: 6  | time in 0 minutes, 16 seconds\n",
    "\n",
    "        Loss: 0.069(train)      |       0.394(valid)\n",
    "        Acc: 98.5%(train)       |       91.9%(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc = train_evaluate_func(model, test_iterator, loss_func, status='test')\n",
    "print(f'\\tLoss: {test_loss:.3f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the results of test dataset...\n",
    "\n",
    "        Loss: 0.402(test)       |       Acc: 92.1%(test)\n",
    "        \n",
    "The results are consistent with the reference paper [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a random news\n",
    "\n",
    "Use the best model so far and test a golf news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "def predict_label(text_string, ngrams):\n",
    "    model.eval()\n",
    "    text_string = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text_string)\n",
    "    tokens = text_string.split(\" \")\n",
    "    ngrams_tokens = torchtext.datasets.TextClassificationDataset.generate_ngrams(tokens, ngrams)\n",
    "    indexed = [txt_cls.fields['text'].vocab.stoi[item] for item in ngrams_tokens]\n",
    "    indexed_tensor = torch.LongTensor(indexed).to(device).unsqueeze(1)\n",
    "    result = model(indexed_tensor)\n",
    "    label_index = result.max(1, keepdim=True)[1].item() + 1 # label starts from 1\n",
    "    return ag_news_label[label_index]\n",
    "\n",
    "example_text_string = \"Defending champion Bryson DeChambeau and 2017 \\\n",
    "                       champion Dustin Johnson have committed to play \\\n",
    "                       in THE NORTHERN TRUST this August for the first \\\n",
    "                       event of the FedExCup Playoffs.\"\n",
    "\n",
    "new_label = \"\"\n",
    "with  open(\"text_classification.pt\", 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    news_label = predict_label(example_text_string, 2)\n",
    "\n",
    "print(\"This is a %s news\" %news_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Sports news"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
