{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Example:\n",
    "\n",
    "This example reproduces the sentiment analysis with the fast text classifier described in [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759). In the papaer, the sentences are represent as bag of words (BoW) and train a linear classifier. Recently, a few text classification datasets, including \n",
    "    - AG_NEWS,\n",
    "    - SogouNews, \n",
    "    - DBpedia, \n",
    "    - YelpReviewPolarity,\n",
    "    - YelpReviewFull, \n",
    "    - YahooAnswers, \n",
    "    - AmazonReviewPolarity,\n",
    "    - AmazonReviewFull\n",
    "are added to PyTorch/torchtext and can be loaded with a single command. This example shows the applciation of TextClassificationDataset and reproduce the results of the paper.\n",
    "\n",
    "## Load data with ngrams\n",
    "A bag of ngrams features is used in the paper to capture some partial information about the local word order. In practice, bi-gram or tri-gram are applied to provide more benefits as word groups than only one word. An example:\n",
    "\n",
    "    \"load data with ngrams\"\n",
    "    Bi-grams results: \"load data\", \"data with\", \"with ngrams\"\n",
    "    Tri-grams results: \"load data with\", \"data with ngrams\"\n",
    "\n",
    "TextClassificationDataset supports the ngrams method. By setting ngrams to 2, the example text in the dataset will be a list of single words plus bi-grams string.\n",
    "\n",
    "Data iterators are loaded via the iters() function in the instance with a batch size of 128 and computation device. At the same time, the word strings are numericalized (i.e. converted from a list of tokens to a list of indexs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import AG_NEWS\n",
    "NGRAMS = 2\n",
    "txt_cls = AG_NEWS(ngrams=NGRAMS)\n",
    "BATCH_SIZE = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "The model is composed of the embedding layer and the linear layer. Between the two layers, we apply a 2D average pooling function over the input signal (see [avg_pool2d](https://pytorch.org/docs/stable/nn.html?highlight=avg_pool2d#torch.nn.AvgPool2d)). Then, we use the log-softmax function to compute the probability distribution over the classes.\n",
    "<img src=\"./pictures/text_sentiment_ngrams_model.png\" width=\"600\" height=\"360\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate an instance\n",
    "\n",
    "The AG_NEWS dataset has four labels and therefore the number of classes is four.\n",
    "\n",
    "    1 : World\n",
    "    2 : Sports\n",
    "    3 : Business\n",
    "    4 : Sci/Tec\n",
    "\n",
    "The vocab size is equal to the length of vocab (including single word and ngrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(txt_cls.fields['text'].vocab)\n",
    "EMBED_DIM = 128\n",
    "NUN_CLASS = 4\n",
    "UNK_IDX = txt_cls.fields['text'].vocab.stoi[txt_cls.fields['text'].unk_token]\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBED_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Add optimizer and loss function\n",
    "\n",
    "[Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) algorithm is used to optimize the model. \n",
    "[NLLLoss](https://pytorch.org/docs/stable/nn.html?highlight=nll_loss#torch.nn.NLLLoss) function calculates the negative log likelihood loss, which is used to train a classification problem with C classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=4.0)\n",
    "loss_func = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to generate batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions generate_batch() and generate_offsets() are used to generate data batches and offsets, which are compatible with EmbeddingBag. EmbeddingBad is efficient to process a sequence of tensors with different lengths so that no padding is required here. This is faster than the origin method depending on TorchText Iterator and Batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_offsets(data_batch):\n",
    "    offsets = [0]\n",
    "    for entry in data_batch:\n",
    "        offsets.append(offsets[-1] + len(entry))\n",
    "    offsets = torch.tensor(offsets[:-1])\n",
    "    return offsets\n",
    "\n",
    "def generate_batch(examples, i, batch_size):\n",
    "    text_batch, label_batch = [], []\n",
    "    for idx in range(i, min(i + batch_size, len(examples))):\n",
    "        text_batch.append(examples[idx][0])\n",
    "        label_batch.append(examples[idx][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to train the model and evaluate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_evaluate_func(model, examples, loss_func, device, batch_size=64, status='train', optimizer=None):\n",
    "\n",
    "    _loss = 0\n",
    "    _acc = 0\n",
    "\n",
    "    if status == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        text, offsets, label = generate_batch(examples, i, batch_size)\n",
    "        if status == 'train':\n",
    "            optimizer.zero_grad()\n",
    "        output = model(text, offsets)\n",
    "        loss = loss_func(output, label)\n",
    "        _loss += loss.item()\n",
    "\n",
    "        if status == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        _acc += (output.argmax(1) == label).sum().item()\n",
    "\n",
    "    return _loss / len(examples), _acc / len(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalize text and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples_tensors = []\n",
    "for idx in range(len(txt_cls.train_examples)):\n",
    "    text = txt_cls.fields['text'].numericalize([txt_cls.train_examples[idx].text])\n",
    "    label = torch.Tensor([txt_cls.train_examples[idx].label - 1]).long()\n",
    "    train_examples_tensors.append([text, label])\n",
    "\n",
    "test_examples_tensors = []\n",
    "for idx in range(len(txt_cls.test_examples)):\n",
    "    text = txt_cls.fields['text'].numericalize([txt_cls.test_examples[idx].text])\n",
    "    label = torch.Tensor([txt_cls.test_examples[idx].label - 1]).long()\n",
    "    test_examples_tensors.append([text, label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "N_EPOCHS = 6\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    import random\n",
    "    random.shuffle(train_examples_tensors)\n",
    "    split_idx = int(len(train_examples_tensors) * 0.7)\n",
    "    train_examples, valid_examples = train_examples_tensors[:split_idx], train_examples_tensors[split_idx:]\n",
    "    \n",
    "    train_loss, train_acc = train_evaluate_func(model, train_examples, loss_func, device=device, status='train', optimizer=optimizer)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = train_evaluate_func(model, valid_examples, loss_func, device=device, status='valid')\n",
    "    \n",
    "    _secs = int(time.time() - start_time)\n",
    "    _mins = _secs / 60\n",
    "    _secs = _secs % 60\n",
    "\n",
    "    if valid_loss < min_valid_loss:\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(model, 'text_classification.pt')\n",
    "    \n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(_mins, _secs))\n",
    "    print(f'\\tLoss: {train_loss:.3f}(train)\\t|\\t{valid_loss:.3f}(valid)')\n",
    "    print(f'\\tAcc: {train_acc * 100:.1f}%(train)\\t|\\t{valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model on GPU with the following information:\n",
    "\n",
    "Epoch: 1  | time in 12 minutes, 33 seconds\n",
    "\n",
    "        Loss: 0.009(train)      |       0.006(valid)\n",
    "        Acc: 77.8%(train)       |       87.4%(valid\n",
    "        \n",
    "Epoch: 2  | time in 18 minutes, 31 seconds\n",
    "\n",
    "        Loss: 0.005(train)      |       0.005(valid\n",
    "        Acc: 89.7%(train)       |       90.2%(valid\n",
    "        \n",
    "Epoch: 3  | time in 10 minutes, 11 seconds\n",
    "\n",
    "        Loss: 0.004(train)      |       0.004(valid)\n",
    "        Acc: 91.9%(train)       |       91.8%(valid)\n",
    "        \n",
    "Epoch: 4  | time in 10 minutes, 55 seconds          \n",
    "\n",
    "        Loss: 0.003(train)      |       0.003(valid)\n",
    "        Acc: 93.4%(train)       |       92.8%(valid)\n",
    "        \n",
    "Epoch: 5  | time in 10 minutes, 57 seconds         \n",
    "\n",
    "        Loss: 0.003(train)      |       0.003(valid)\n",
    "        Acc: 94.5%(train)       |       94.7%(valid)\n",
    "        \n",
    "Epoch: 6  | time in 18 minutes, 19 seconds          \n",
    "\n",
    "        Loss: 0.002(train)      |       0.002(valid)\n",
    "        Acc: 95.6%(train)       |       95.3%(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "with torch.no_grad():\n",
    "    random.shuffle(test_examples_tensors)\n",
    "    test_loss, test_acc = train_evaluate_func(model, test_examples_tensors, loss_func, device=device, status='test')\n",
    "print(f'\\tLoss: {test_loss:.3f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the results of test dataset...\n",
    "\n",
    "        Loss: 0.005(test)       |       Acc: 90.4%(test)\n",
    "        \n",
    "The results are consistent with the reference paper [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a random news\n",
    "\n",
    "Use the best model so far and test a golf news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from torchtext.data.utils import generate_ngrams\n",
    "\n",
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "def predict_label(text_string, ngrams):\n",
    "    model.eval()\n",
    "    text_string = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text_string)\n",
    "    tokens = text_string.split(\" \")\n",
    "    ngrams_tokens = generate_ngrams(tokens, ngrams)\n",
    "    indexed = [txt_cls.fields['text'].vocab.stoi[item] for item in ngrams_tokens]\n",
    "    indexed_tensor = torch.LongTensor(indexed).to(device)\n",
    "    offsets = torch.Tensor([0]).long().to(device)\n",
    "    result = model(indexed_tensor, offsets)\n",
    "    label_index = result.argmax(1).item() + 1\n",
    "    return ag_news_label[label_index]\n",
    "\n",
    "example_text_string = \"Defending champion Bryson DeChambeau and 2017 \\\n",
    "    champion Dustin Johnson have committed to play in THE NORTHERN \\\n",
    "    TRUST this August for the first event of the FedExCup Playoffs. \\\n",
    "    DeChambeau and Johnson will be joined by the biggest names in the \\\n",
    "    game at Liberty National Golf Club, which gets ready to host THE \\\n",
    "    NORTHERN TRUST for the third time (previously 2009 and 2013). Other \\\n",
    "    notable past champions of THE NORTHERN TRUST include Adam \\\n",
    "    Scott (who won in 2013), Sergio Garcia, Jason Day, Matt \\\n",
    "    Kuchar and Patrick Reed.\"\n",
    "\n",
    "new_label = \"\"\n",
    "with  open(\"text_classification.pt\", 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    news_label = predict_label(example_text_string, 2)\n",
    "\n",
    "print(\"This is a %s news\" %news_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Sports news"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
