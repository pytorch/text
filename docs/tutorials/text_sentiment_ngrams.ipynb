{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Example:\n",
    "\n",
    "This example shows how to use the text classification datasets, including  \n",
    "\n",
    "    - AG_NEWS,\n",
    "    - SogouNews, \n",
    "    - DBpedia, \n",
    "    - YelpReviewPolarity,\n",
    "    - YelpReviewFull, \n",
    "    - YahooAnswers, \n",
    "    - AmazonReviewPolarity,\n",
    "    - AmazonReviewFull\n",
    "\n",
    "Those datasets are added to TorchText and can be loaded with a single command. \n",
    " \n",
    "This example shows the applciation of TextClassification Dataset. It reproduces the sentiment analysis with the fast text classifier described in [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759). In the papaer, the sentences are represent as bag of words (BoW) and train a linear classifier. \n",
    "\n",
    "\n",
    "## Load data with ngrams\n",
    "A bag of ngrams features is used in the paper to capture some partial information about the local word order. In practice, bi-gram or tri-gram are applied to provide more benefits as word groups than only one word. An example:\n",
    "\n",
    "    \"load data with ngrams\"\n",
    "    Bi-grams results: \"load data\", \"data with\", \"with ngrams\"\n",
    "    Tri-grams results: \"load data with\", \"data with ngrams\"\n",
    "\n",
    "TextClassificationDataset supports the ngrams method. By setting ngrams to 2, the example text in the dataset will be a list of single words plus bi-grams string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "NGRAMS = 2\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "The model is composed of the embeddingbag layer and the linear layer (see the figure below)\n",
    "<img src=\"./pictures/text_sentiment_ngrams_model.png\" width=\"600\" height=\"360\">. nn.EmbeddingBag computes the mean of 'bags' of embeddings. Since it doesn't instantiate the intermediate embeddings, nn.EmbeddingBag can enhance the performance and memory efficiency to process a sequence of tensors. Additionally, the text entries here have different lengths. nn.EmbeddingBag requires no padding here so this method is much faster than the original one with TorchText Iterator and Batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate an instance\n",
    "\n",
    "The AG_NEWS dataset has four labels and therefore the number of classes is four.\n",
    "\n",
    "    1 : World\n",
    "    2 : Sports\n",
    "    3 : Business\n",
    "    4 : Sci/Tec\n",
    "\n",
    "The vocab size is equal to the length of vocab (including single word and ngrams). The number of classes is equal to the number of labels, which is four in AG_NEWS case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 256\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Add optimizer and loss function\n",
    "\n",
    "[SGD](https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.SGD) algorithm is used to optimize the model. \n",
    "[CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. It is useful when training a classification problem with C classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=4.0)\n",
    "loss_func = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to generate batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the text entries have different lengths, a custom function generate_batch() is used to generate data batches and offsets, which are compatible with EmbeddingBag. The function is passed to 'collate_fn' in torch.utils.data.DataLoader. The input to 'collate_fn' is a list of tensors with the size of batch_size, and the 'collate_fn' function packs them into a mini-batch. Pay attention here and make sure that 'collate_fn' is declared as a top level def. This ensures that the function is available in each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "\n",
    "    def generate_offsets(data_batch):\n",
    "        offsets = [0]\n",
    "        for entry in data_batch:\n",
    "            offsets.append(offsets[-1] + len(entry))\n",
    "        offsets = torch.tensor(offsets[:-1])\n",
    "        return offsets\n",
    "\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = generate_offsets(text)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to train the model and evaluate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.utils.data.DataLoader is recommended for PyTorch domain libraries. We use DataLoader here to load AG_NEWS datasets and send it to the model for training/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_evaluate_func(model, dataset, loss_func, device, batch_size=64, status='train', optimizer=None):\n",
    "\n",
    "    _loss = 0\n",
    "    _acc = 0\n",
    "\n",
    "    if status == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    data = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                      collate_fn=generate_batch, num_workers=1)\n",
    "\n",
    "    for i, (text, offsets, label) in enumerate(data):\n",
    "        if status == 'train':\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        text, offsets, label = text.to(device), offsets.to(device), label.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = loss_func(output, label)\n",
    "        _loss += loss.item()\n",
    "\n",
    "        if status == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        _acc += (output.argmax(1) == label).sum().item()\n",
    "\n",
    "    return _loss / len(dataset), _acc / len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset and run the model\n",
    "Since the original AG_NEWS has no valid dataset, we have to split the training dataset into train/valid sets with the split ratios of 0.7 (train) and 0.3 (valid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 12\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_len = int(len(train_dataset) * 0.7)\n",
    "    sub_train_dataset, sub_valid_dataset = random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "    train_loss, train_acc = train_evaluate_func(model, sub_train_dataset, loss_func,\n",
    "                                                device=device, status='train', optimizer=optimizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = train_evaluate_func(model, sub_valid_dataset, loss_func,\n",
    "                                                    device=device, status='valid')\n",
    "    \n",
    "    _secs = int(time.time() - start_time)\n",
    "    _mins = _secs / 60\n",
    "    _secs = _secs % 60\n",
    "\n",
    "    if valid_loss < min_valid_loss:\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(model, 'text_classification.pt')\n",
    "    \n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(_mins, _secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\t{valid_loss:.4f}(valid)')\n",
    "    print(f'\\tAcc: {train_acc * 100:.1f}%(train)\\t|\\t{valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model on GPU with the following information:\n",
    "\n",
    "Epoch: 1  | time in 0 minutes, 5 seconds\n",
    "\n",
    "        Loss: 0.0088(train)     |       0.0064(valid)\n",
    "        Acc: 79.9%(train)       |       85.7%(valid)\n",
    "        \n",
    "Epoch: 2  | time in 0 minutes, 5 seconds\n",
    "\n",
    "        Loss: 0.0048(train)     |       0.0046(valid)\n",
    "        Acc: 89.8%(train)       |       90.2%(valid)\n",
    "        \n",
    "Epoch: 3  | time in 0 minutes, 5 seconds\n",
    "\n",
    "        Loss: 0.0038(train)     |       0.0039(valid)\n",
    "        Acc: 92.1%(train)       |       91.6%(valid)\n",
    "        \n",
    "Epoch: 4  | time in 0 minutes, 5 seconds\n",
    "\n",
    "        Loss: 0.0031(train)     |       0.0032(valid)\n",
    "        Acc: 93.5%(train)       |       93.1%(valid)\n",
    "        \n",
    "Epoch: 5  | time in 0 minutes, 5 seconds\n",
    "\n",
    "        Loss: 0.0026(train)     |       0.0028(valid)\n",
    "        Acc: 94.7%(train)       |       94.2%(valid)\n",
    "        \n",
    "Epoch: 6  | time in 0 minutes, 5 seconds\n",
    "\n",
    "        Loss: 0.0022(train)     |       0.0021(valid)\n",
    "        Acc: 95.8%(train)       |       95.8%(valid)\n",
    "        \n",
    "Epoch: 7  | time in 0 minutes, 5 seconds\n",
    "\n",
    "        Loss: 0.0018(train)     |       0.0018(valid)\n",
    "        Acc: 96.6%(train)       |       96.3%(valid)\n",
    "        \n",
    "Epoch: 8  | time in 0 minutes, 5 seconds\n",
    "\n",
    "        Loss: 0.0014(train)     |       0.0014(valid)\n",
    "        Acc: 97.4%(train)       |       97.3%(valid)\n",
    "        \n",
    "Epoch: 9  | time in 0 minutes, 5 seconds\n",
    "\n",
    "        Loss: 0.0011(train)     |       0.0013(valid)\n",
    "        Acc: 98.0%(train)       |       97.7%(valid)\n",
    "        \n",
    "Epoch: 10  | time in 0 minutes, 5 seconds\n",
    "\n",
    "        Loss: 0.0009(train)     |       0.0010(valid)\n",
    "        Acc: 98.5%(train)       |       98.3%(valid)\n",
    "        \n",
    "Epoch: 11  | time in 0 minutes, 5 seconds\n",
    "\n",
    "        Loss: 0.0008(train)     |       0.0009(valid)\n",
    "        Acc: 98.8%(train)       |       98.3%(valid)\n",
    "        \n",
    "Epoch: 12  | time in 0 minutes, 5 seconds\n",
    "\n",
    "        Loss: 0.0006(train)     |       0.0009(valid)\n",
    "        Acc: 99.1%(train)       |       98.3%(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc = train_evaluate_func(model, test_dataset, loss_func,\n",
    "                                              device=device, status='test')\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the results of test dataset...\n",
    "\n",
    "        Loss: 0.005(test)       |       Acc: 90.4%(test)\n",
    "        \n",
    "The results are consistent with the reference paper [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a random news\n",
    "\n",
    "Use the best model so far and test a golf news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "\n",
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "vocab = train_dataset._vocab\n",
    "\n",
    "def predict_label(text_string, ngrams):\n",
    "    model.eval()\n",
    "    text_string = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text_string)\n",
    "    tokens = text_string.split(\" \")\n",
    "    ngrams_tokens = ngrams_iterator(tokens, ngrams)\n",
    "    indexed_tensor = torch.tensor([vocab[token] for token in ngrams_tokens]).to(device)\n",
    "    offsets = torch.Tensor([0]).long().to(device)\n",
    "    result = model(indexed_tensor, offsets)\n",
    "    label_index = result.argmax(1).item() + 1\n",
    "    return ag_news_label[label_index]\n",
    "\n",
    "example_text_string = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "new_label = \"\"\n",
    "with open(\"text_classification.pt\", 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    news_label = predict_label(example_text_string, 1)\n",
    "\n",
    "print(\"This is a %s news\" %news_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Sports news"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
