{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchText Dataset Loading and Procesing Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a framework for deep learning problems, PyTorch supports many tools for data loading and processing. In the past, TorchText datasets were developed and maintained depending on its own utils (like [Iterator](https://github.com/pytorch/text/blob/284a51651dd9697f9afd76f2ceb23a8181ae7552/torchtext/data/iterator.py#L15)/[Batch](https://github.com/pytorch/text/blob/284a51651dd9697f9afd76f2ceb23a8181ae7552/torchtext/data/batch.py#L4)). In this tutorial, we will introduce [torch.utils.data.DataLoader](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py) and its application to load TorchText dataset. The PyTorch dataloader is widely used by the commnunity, especially [TorchVision](https://github.com/pytorch/vision/tree/master/torchvision), and actively maintained by the developers. We hope to apply PyTorch DataLoader for the future TorchText datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TorchText Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AG_NEWS dataset has recently been added to TorchText for supervised learning. The original dataset has the training and testing examples, and each example contains a list of tokens and a label. By setting the parameter \"ngrams\", the token list includes a contiguous sequence of n items from a given sample of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from text_classification import AG_NEWS\n",
    "txt_cls = AG_NEWS(ngrams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the train_examples into train and valid set with a ratio of 0.7 (train) and 0.3 (valid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = torchtext.data.dataset.RandomShuffler(None)\n",
    "train_examples, _test_examples, valid_examples = \\\n",
    "    torchtext.data.dataset.rationed_split(txt_cls.train_examples, 0.7, 0.0, 0.3, rnd)\n",
    "test_examples = txt_cls.test_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert into dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset for the text problem. The text data (i.e., \"examples\") and corresponding processors (i.e., \"fields\") are stored as in the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, examples, fields):\n",
    "        self.examples = examples\n",
    "        self.fields = fields\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        try:\n",
    "            return len(self.examples)\n",
    "        except TypeError:\n",
    "            return 2**32\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self.examples:\n",
    "            yield x\n",
    "\n",
    "train_dataset = TextDataset(train_examples, txt_cls.fields)\n",
    "test_dataset = TextDataset(test_examples, txt_cls.fields)\n",
    "valid_dataset = TextDataset(valid_examples, txt_cls.fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad and Numericalize Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, the data saved in the examples of TextDataset are still a list of tokens. In order to use PyTorch DataLoader, we have to pad the token lists with the same length and convert them into PyTorch tensors. It should be noted that the output of [Field.pad](https://github.com/pytorch/text/blob/284a51651dd9697f9afd76f2ceb23a8181ae7552/torchtext/data/field.py#L240) and [Field.numericalize](https://github.com/pytorch/text/blob/284a51651dd9697f9afd76f2ceb23a8181ae7552/torchtext/data/field.py#L311) functions is in the shape of [seq_length * N] where seq_length is the length of token lists and N is the number of examples. They have to be transposed to the shape of [N * seq_length]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_numericalize(dataset, device=None):\n",
    "\n",
    "    def convert_examples_to_dict(examples, attr_name_list):\n",
    "\n",
    "        examples_dict = {att: [] for att in attr_name_list}\n",
    "\n",
    "        for ex in examples:\n",
    "            for name in attr_name_list:\n",
    "                if hasattr(ex, name):\n",
    "                    examples_dict[name].append(ex.__dict__[name])\n",
    "                else:\n",
    "                    print(\"no attribute found: \", name)\n",
    "\n",
    "        return examples_dict\n",
    "    \n",
    "    examples, fields = convert_examples_to_dict(dataset.examples,\n",
    "                                                ['text', 'label']), dataset.fields\n",
    "\n",
    "    # The output of pad_and_numericalize function is in the shape of\n",
    "    # [src_length * N]\n",
    "    # Transpose into the shape of\n",
    "    # [N * src_length]\n",
    "    examples['text'] = fields['text'].numericalize(fields['text'].pad(examples['text']),\n",
    "                                                   device=device).transpose(0, 1)\n",
    "    examples['label'] = fields['label'].numericalize(fields['label'].\n",
    "                                                     pad(examples['label']),\n",
    "                                                     device=device)\n",
    "    dataset.examples = [{'text': text, 'label': label} for (text, label)\n",
    "                        in zip(examples['text'], examples['label'])]\n",
    "    return dataset\n",
    "\n",
    "# Pad/numericalize train/test/valid data\n",
    "train_dataset = pad_and_numericalize(train_dataset, device=torch.device(\"cpu\"))\n",
    "test_dataset = pad_and_numericalize(test_dataset, device=torch.device(\"cpu\"))\n",
    "valid_dataset = pad_and_numericalize(valid_dataset, device=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add to DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply torch.utils.data.DataLoader on AG_NEWS dataset. Instead of existing [Iterator](https://github.com/pytorch/text/blob/284a51651dd9697f9afd76f2ceb23a8181ae7552/torchtext/data/iterator.py#L15)/[Batch](https://github.com/pytorch/text/blob/284a51651dd9697f9afd76f2ceb23a8181ae7552/torchtext/data/batch.py#L4) classes in TorchText, this tutorial applies PyTorch [DataLoader](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py). torch.utils.data.DataLoader is an iterator which provides the following features:\n",
    "- Batching the data\n",
    "- Shuffling the data\n",
    "- Load the data in parallel using multiprocessing workers\n",
    "- Save/load datasets for re-using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset_dataloader = DataLoader(train_dataset, batch_size=128,\n",
    "                                      shuffle=True, num_workers=4)\n",
    "test_dataset_dataloader = DataLoader(test_dataset, batch_size=128,\n",
    "                                     shuffle=True, num_workers=4)\n",
    "valid_dataset_dataloader = DataLoader(valid_dataset, batch_size=128,\n",
    "                                      shuffle=True, num_workers=4)\n",
    "for i_batch, sample_batched in enumerate(train_dataset_dataloader):\n",
    "    print(i_batch, sample_batched['text'].size(), sample_batched['label'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/loader dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the post-processing text data have been demanded for a long time by TorchText user community because data processing usually is time-consuming. With PyTorch DataLoader, we can apply the standard torch save/load methods and re-used post-processing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save([train_dataset_dataloader, test_dataset_dataloader, valid_dataset_dataloader],\n",
    "           './dataloader_text_cls_example.pt')\n",
    "dataloader_train, dataloader_test, dataloader_valid = torch.load('./dataloader_text_cls_example.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
