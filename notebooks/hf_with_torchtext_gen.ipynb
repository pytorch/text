{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, you will need to install the huggingface library with the following command: `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jrcummings/miniconda/envs/torchtext/lib/python3.9/site-packages/tqdm-4.64.1-py3.9.egg/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, BartForConditionalGeneration, BartTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torchtext.prototype.generate import GenerationUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jrcummings/miniconda/envs/torchtext/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['owning a dog is good for you, according to studies. a dog is']\n"
     ]
    }
   ],
   "source": [
    "# Testing Huggingface's T5\n",
    "test_sequence = [\"summarize: studies have shown that owning a dog is good for you\"]\n",
    "generative_hf_t5 = GenerationUtil(t5, is_encoder_decoder=True, is_huggingface_model=True)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "test_sequence_tk = t5_tokenizer(test_sequence, return_tensors=\"pt\").input_ids\n",
    "tokens = generative_hf_t5.generate(test_sequence_tk, max_len=20, pad_idx=t5.config.pad_token_id)\n",
    "print(t5_tokenizer.batch_decode(tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a dog is good for you. studies have shown that dog ownership is good for your overall health and well-being.']\n"
     ]
    }
   ],
   "source": [
    "# Testing HuggingFace's T5 w/ Beam Search\n",
    "tokens = generative_hf_t5.generate(test_sequence_tk, max_len=100, pad_idx=t5.config.pad_token_id, num_beams=5, beam_size_token=t5.config.vocab_size)\n",
    "print(t5_tokenizer.batch_decode(tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a dog is good for you. studies have shown that dog ownership is good for your overall health and well-being.'] 9.786320924758911\n",
      "['studies have shown that owning a dog is good for you. studies have shown that owning a dog is good for you.'] 1.3000121116638184\n"
     ]
    }
   ],
   "source": [
    "# Testing Decoding Speed HuggingFace's T5 w/ TorchText Beam Search vs. HuggingFace Beam Search\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "tokens = generative_hf_t5.generate(test_sequence_tk, max_len=100, pad_idx=t5.config.pad_token_id, num_beams=5, beam_size_token=t5.config.vocab_size)\n",
    "end = time.time()\n",
    "print(t5_tokenizer.batch_decode(tokens, skip_special_tokens=True), end - start)\n",
    "\n",
    "start = time.time()\n",
    "tokens = t5.generate(test_sequence_tk, max_length=100, num_beams=5, do_sample=False)\n",
    "end = time.time()\n",
    "print(t5_tokenizer.batch_decode(tokens, skip_special_tokens=True), end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PG. PG&E said it scheduled the blackouts in response to forecasts for high winds.']\n"
     ]
    }
   ],
   "source": [
    "# Testing Huggingface's BART\n",
    "test_sequence = [\"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"]\n",
    "generative_hf_bart = GenerationUtil(bart, is_encoder_decoder=True, is_huggingface_model=True)\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "test_sequence_tk = bart_tokenizer(test_sequence, return_tensors=\"pt\").input_ids\n",
    "tokens = generative_hf_bart.generate(test_sequence_tk, max_len=20, pad_idx=bart.config.pad_token_id)\n",
    "print(bart_tokenizer.batch_decode(tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nearly. PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions.']\n"
     ]
    }
   ],
   "source": [
    "tokens = generative_hf_bart.generate(test_sequence_tk, max_len=20, pad_idx=bart.config.pad_token_id, num_beams=5, beam_size_token=bart.config.vocab_size)\n",
    "print(bart_tokenizer.batch_decode(tokens, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs. The blackouts are expected to last through at least midday tomorrow. to be affected by the shutoffs which were expected to last through at least midday tomorrow. to be affected by the shutoffs which were expected to last through at least midday tomorrow. to be affected by the'] 58.09997892379761\n",
      "['PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs. The blackouts were expected to last through at least midday tomorrow.'] 2.456479787826538\n"
     ]
    }
   ],
   "source": [
    "# Testing Decoding Speed HuggingFace's BART w/ TorchText Beam Search vs. HuggingFace Beam Search\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "tokens = generative_hf_bart.generate(test_sequence_tk, max_len=100, pad_idx=t5.config.pad_token_id, num_beams=5, eos_score=1.0, beam_size_token=t5.config.vocab_size)\n",
    "end = time.time()\n",
    "print(bart_tokenizer.batch_decode(tokens, skip_special_tokens=True), end - start)\n",
    "\n",
    "start = time.time()\n",
    "tokens = bart.generate(test_sequence_tk, max_length=100, num_beams=5, do_sample=False)\n",
    "end = time.time()\n",
    "print(bart_tokenizer.batch_decode(tokens, skip_special_tokens=True), end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to\"]\n"
     ]
    }
   ],
   "source": [
    "# Testing Huggingface's GPT2\n",
    "test_sequence = [\"I enjoy walking with my cute dog\"]\n",
    "generative_hf_gpt2 = GenerationUtil(gpt2, is_encoder_decoder=False, is_huggingface_model=True)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "test_sequence_tk = gpt2_tokenizer(test_sequence, return_tensors=\"pt\").input_ids\n",
    "tokens = generative_hf_gpt2.generate(test_sequence_tk, max_len=20, pad_idx=gpt2.config.pad_token_id)\n",
    "print(gpt2_tokenizer.batch_decode(tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I enjoy walking with my cute dog,\" says Kelli Williams-Petersen. The dog loves it so much, that when she']\n"
     ]
    }
   ],
   "source": [
    "tokens = generative_hf_gpt2.generate(test_sequence_tk, max_len=20, pad_idx=gpt2.config.pad_token_id, num_beams=5, beam_size_token=gpt2.config.vocab_size)\n",
    "print(gpt2_tokenizer.batch_decode(tokens, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1851d106532ddfc6fbd983b9ae95397243fcc3930d811046c990ea169e960650"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
