{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# SST-2 Binary text classification with XLM-RoBERTa model\n\n**Author**: `Parmeet Bhatia <parmeetbhatia@fb.com>`__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n\nThis tutorial demonstrates how to train a text classifier on SST-2 binary dataset using a pre-trained XLM-RoBERTa (XLM-R) model.\nWe will show how to use torchtext library to:\n\n1. build text pre-processing pipeline for XLM-R model\n2. read SST-2 dataset and transform it using text and label transformation\n3. instantiate classification model using pre-trained XLM-R encoder\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n\nDEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Transformation\n\nModels like XLM-R cannot work directly with raw text. The first step in training\nthese models is to transform input text into tensor (numerical) form such that it\ncan then be processed by models to make predictions. A standard way to process text is:\n\n1. Tokenize text\n2. Convert tokens into (integer) IDs\n3. Add any special tokens IDs\n\nXLM-R uses sentencepiece model for text tokenization. Below, we use pre-trained sentencepiepce\nmodel along with corresponding vocabulary to build text pre-processing pipeline using torchtext's transforms.\nThe transforms are pipelined using :py:func:`torchtext.transforms.Sequential` which is similar to :py:func:`torch.nn.Sequential`\nbut is torchscriptable. Note that the transforms support both batched and non-batched text inputs i.e, one\ncan either pass a single sentence or list of sentences.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torchtext.transforms as T\nfrom torch.hub import load_state_dict_from_url\n\npadding_idx = 1\nbos_idx = 0\neos_idx = 2\nmax_seq_len = 256\nxlmr_vocab_path = r\"https://download.pytorch.org/models/text/xlmr.vocab.pt\"\nxlmr_spm_model_path = r\"https://download.pytorch.org/models/text/xlmr.sentencepiece.bpe.model\"\n\ntext_transform = T.Sequential(\n    T.SentencePieceTokenizer(xlmr_spm_model_path),\n    T.VocabTransform(load_state_dict_from_url(xlmr_vocab_path)),\n    T.Truncate(max_seq_len - 2),\n    T.AddToken(token=bos_idx, begin=True),\n    T.AddToken(token=eos_idx, begin=False),\n)\n\n\nfrom torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternately we can also use transform shipped with pre-trained model that does all of the above out-of-the-box\n\n::\n\n  text_transform = XLMR_BASE_ENCODER.transform()\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\ntorchtext provides several standard NLP datasets. For complete list, refer to documentation\nat https://pytorch.org/text/stable/datasets.html. These datasets are build using composable torchdata\ndatapipes and hence support standard flow-control and mapping/transformation using user defined functions\nand transforms. Below, we demonstrate how to use text and label processing transforms to pre-process the\nSST-2 dataset.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Using datapipes is still currently subject to a few caveats. If you wish\n      to extend this example to include shuffling, multi-processing, or\n      distributed learning, please see `this note <datapipes_warnings>`\n      for further instructions.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import SST2\n\nbatch_size = 16\n\ntrain_datapipe = SST2(split=\"train\")\ndev_datapipe = SST2(split=\"dev\")\n\n\n# Transform the raw dataset using non-batched API (i.e apply transformation line by line)\ndef apply_transform(x):\n    return text_transform(x[0]), x[1]\n\n\ntrain_datapipe = train_datapipe.map(apply_transform)\ntrain_datapipe = train_datapipe.batch(batch_size)\ntrain_datapipe = train_datapipe.rows2columnar([\"token_ids\", \"target\"])\ntrain_dataloader = DataLoader(train_datapipe, batch_size=None)\n\ndev_datapipe = dev_datapipe.map(apply_transform)\ndev_datapipe = dev_datapipe.batch(batch_size)\ndev_datapipe = dev_datapipe.rows2columnar([\"token_ids\", \"target\"])\ndev_dataloader = DataLoader(dev_datapipe, batch_size=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternately we can also use batched API (i.e apply transformation on the whole batch)\n\n::\n\n  def batch_transform(x):\n      return {\"token_ids\": text_transform(x[\"text\"]), \"target\": x[\"label\"]}\n\n\n  train_datapipe = train_datapipe.batch(batch_size).rows2columnar([\"text\", \"label\"])\n  train_datapipe = train_datapipe.map(lambda x: batch_transform)\n  dev_datapipe = dev_datapipe.batch(batch_size).rows2columnar([\"text\", \"label\"])\n  dev_datapipe = dev_datapipe.map(lambda x: batch_transform)\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Preparation\n\ntorchtext provides SOTA pre-trained models that can be used to fine-tune on downstream NLP tasks.\nBelow we use pre-trained XLM-R encoder with standard base architecture and attach a classifier head to fine-tune it\non SST-2 binary classification task. We shall use standard Classifier head from the library, but users can define\ntheir own appropriate task head and attach it to the pre-trained encoder. For additional details on available pre-trained models,\nplease refer to documentation at https://pytorch.org/text/main/models.html\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_classes = 2\ninput_dim = 768\n\nfrom torchtext.models import RobertaClassificationHead, XLMR_BASE_ENCODER\n\nclassifier_head = RobertaClassificationHead(num_classes=num_classes, input_dim=input_dim)\nmodel = XLMR_BASE_ENCODER.get_model(head=classifier_head)\nmodel.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training methods\n\nLet's now define the standard optimizer and training criteria as well as some helper functions\nfor training and evaluation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torchtext.functional as F\nfrom torch.optim import AdamW\n\nlearning_rate = 1e-5\noptim = AdamW(model.parameters(), lr=learning_rate)\ncriteria = nn.CrossEntropyLoss()\n\n\ndef train_step(input, target):\n    output = model(input)\n    loss = criteria(output, target)\n    optim.zero_grad()\n    loss.backward()\n    optim.step()\n\n\ndef eval_step(input, target):\n    output = model(input)\n    loss = criteria(output, target).item()\n    return float(loss), (output.argmax(1) == target).type(torch.float).sum().item()\n\n\ndef evaluate():\n    model.eval()\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n    counter = 0\n    with torch.no_grad():\n        for batch in dev_dataloader:\n            input = F.to_tensor(batch[\"token_ids\"], padding_value=padding_idx).to(DEVICE)\n            target = torch.tensor(batch[\"target\"]).to(DEVICE)\n            loss, predictions = eval_step(input, target)\n            total_loss += loss\n            correct_predictions += predictions\n            total_predictions += len(target)\n            counter += 1\n\n    return total_loss / counter, correct_predictions / total_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train\n\nNow we have all the ingredients to train our classification model. Note that we are able to directly iterate\non our dataset object without using DataLoader. Our pre-process dataset  shall yield batches of data already,\nthanks to the batching datapipe we have applied. For distributed training, we would need to use DataLoader to\ntake care of data-sharding.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_epochs = 1\n\nfor e in range(num_epochs):\n    for batch in train_dataloader:\n        input = F.to_tensor(batch[\"token_ids\"], padding_value=padding_idx).to(DEVICE)\n        target = torch.tensor(batch[\"target\"]).to(DEVICE)\n        train_step(input, target)\n\n    loss, accuracy = evaluate()\n    print(\"Epoch = [{}], loss = [{}], accuracy = [{}]\".format(e, loss, accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output\n\n::\n\n  100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|5.07M/5.07M [00:00<00:00, 40.8MB/s]\n  Downloading: \"https://download.pytorch.org/models/text/xlmr.vocab.pt\" to /root/.cache/torch/hub/checkpoints/xlmr.vocab.pt\n  100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|4.85M/4.85M [00:00<00:00, 16.8MB/s]\n  Downloading: \"https://download.pytorch.org/models/text/xlmr.base.encoder.pt\" to /root/.cache/torch/hub/checkpoints/xlmr.base.encoder.pt\n  100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|1.03G/1.03G [00:26<00:00, 47.1MB/s]\n  Epoch = [0], loss = [0.2629831412637776], accuracy = [0.9105504587155964]\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}